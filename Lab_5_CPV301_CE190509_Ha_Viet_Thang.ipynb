{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1WXGnr7pVbOH-7dJOAFqnVRdnpj0gOy2b","authorship_tag":"ABX9TyMKB82deySeHJgwOE0R0Fwx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wUXdUzpPwghO","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1FF-DqUm_kfkV71yq4jWRNRrCyFf7Fhlu"},"executionInfo":{"status":"ok","timestamp":1729054307782,"user_tz":-420,"elapsed":95750,"user":{"displayName":"Michael Ha","userId":"05310127525331572955"}},"outputId":"a7b1eee2-bffe-42d8-820e-48ae547a5fc0"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import cv2\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","\n","def ransac_image_alignment(image1, image2, num_iterations=1000, tolerance=5.0):\n","    # Convert images to grayscale\n","    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n","    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n","\n","    # Detect ORB features and compute descriptors\n","    orb = cv2.ORB_create()\n","    keypoints1, descriptors1 = orb.detectAndCompute(gray1, None)\n","    keypoints2, descriptors2 = orb.detectAndCompute(gray2, None)\n","\n","    # Match features using brute force matcher\n","    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","    matches = bf.match(descriptors1, descriptors2)\n","    matches = sorted(matches, key=lambda x: x.distance)\n","\n","    # Extract matched keypoints\n","    points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches])\n","    points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches])\n","\n","    best_H = None\n","    max_inliers = 0\n","\n","    for _ in range(num_iterations):\n","        # Randomly select 4 points\n","        indices = np.random.choice(len(matches), 4, replace=False)\n","        src_points = points1[indices]\n","        dst_points = points2[indices]\n","\n","        # Compute homography using these points\n","        H, _ = cv2.findHomography(src_points, dst_points, cv2.RANSAC)\n","\n","        if H is not None:\n","            # Warp image1 using the computed homography\n","            warped_image1 = cv2.warpPerspective(image1, H, (image2.shape[1], image2.shape[0]))\n","\n","            # Calculate difference between warped_image1 and image2\n","            diff = cv2.absdiff(warped_image1, image2)\n","            gray_diff = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n","            _, mask = cv2.threshold(gray_diff, tolerance, 255, cv2.THRESH_BINARY_INV)\n","            inliers = cv2.countNonZero(mask)\n","\n","            # Update best homography if the current one has more inliers\n","            if inliers > max_inliers:\n","                best_H = H\n","                max_inliers = inliers\n","\n","    # Warp image1 using the best homography\n","    aligned_image = cv2.warpPerspective(image1, best_H, (image2.shape[1], image2.shape[0]))\n","\n","    return aligned_image, best_H\n","\n","# Align images using RANSAC\n","image1 = cv2.imread(\"/content/GlobalAlignment2.jpg\")\n","image2 = cv2.imread(\"/content/GlobalAlignment1.jpg\")\n","aligned_image, H = ransac_image_alignment(image1, image2)\n","cv2.imwrite(\"result.png\", aligned_image)\n","\n","# Display the result using cv2_imshow\n","cv2_imshow(aligned_image)"]}]}